%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for jaemin at 2015-12-15 14:48:01 +0900 


%% Saved with string encoding Unicode (UTF-8) 



@article{Auke-dirk2008,
	Author = {Auke-dirk, Ted Tom},
	Doi = {10.1109/34.982903},
	File = {::},
	Number = {2},
	Pages = {237--267},
	Title = {{Vision for Mobile Robot Navigation : A Survey Keyfeatures}},
	Volume = {24},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/34.982903}}

@article{Davison2007,
	Author = {Davison, Andrew J and Reid, Ian D and Molton, Nicholas D and Stasse, Olivier},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Davison et al. - Pattern Analysis and Machine Intelligence, IEEE Transactions on - 2007 - MonoSLAM Real-time single camera SLAM.pdf:pdf},
	Journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
	Number = {6},
	Pages = {1052--1067},
	Publisher = {IEEE},
	Title = {{MonoSLAM: Real-time single camera SLAM}},
	Volume = {29},
	Year = {2007}}

@article{Engel2014,
	Abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct meth- ods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
	Author = {Engel, Jakob and Sch, Thomas and Cremers, Daniel},
	Doi = {10.1007/978-3-319-10605-2{\_}54},
	File = {::},
	Isbn = {978-3-319-10604-5},
	Issn = {16113349},
	Journal = {Eccv},
	Pages = {1--16},
	Title = {{LSD-SLAM: Large-Scale Direct Monocular SLAM}},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/978-3-319-10605-2%7B%5C_%7D54}}

@book{Hartley2003,
	Abstract = {A basic problem in computer vision is to understand the structure of a real world scene given several images of it. Techniques for solving this problem are taken from projective geometry and photogrammetry. Here, the authors cover the geometric principles and their algebraic representation in terms of camera projection matrices, the fundamental matrix and the trifocal tensor. The theory and methods of computation of these entities are discussed with real examples, as is their use in the reconstruction of scenes from multiple images. The new edition features an extended introduction covering the key ideas in the book (which itself has been updated with additional examples and appendices) and significant new results which have appeared since the first edition. Comprehensive background material is provided, so readers familiar with linear algebra and basic numerical methods can understand the projective geometry and estimation algorithms presented, and implement the algorithms directly from the book.},
	Author = {Hartley, Richard and Zisserman, Andrew},
	Isbn = {0521540518},
	Title = {{Multiple View Geometry in Computer Vision}},
	Url = {https://books.google.com/books?hl=ko{\&}lr={\&}id=si3R3Pfa98QC{\&}pgis=1},
	Year = {2003},
	Bdsk-Url-1 = {https://books.google.com/books?hl=ko%7B%5C&%7Dlr=%7B%5C&%7Did=si3R3Pfa98QC%7B%5C&%7Dpgis=1}}

@article{Kerl2013,
	Abstract = {In this paper, we propose a dense visual SLAM method for RGB-D cameras that minimizes both the photometric and the depth error over all pixels. In contrast to sparse, feature-based methods, this allows us to better exploit the available information in the image data which leads to higher pose accuracy. Furthermore, we propose an entropy-based similarity measure for keyframe selection and loop closure detection. From all successful matches, we build up a graph that we optimize using the g2o framework. We evaluated our approach extensively on publicly available benchmark datasets, and found that it performs well in scenes with low texture as well as low structure. In direct comparison to several state-of-the-art methods, our approach yields a significantly lower trajectory error. We release our software as open-source.},
	Author = {Kerl, Christian and Sturm, Jurgen and Cremers, Daniel},
	Doi = {10.1109/IROS.2013.6696650},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Kerl, Sturm, Cremers - IEEE International Conference on Intelligent Robots and Systems - 2013 - Dense visual SLAM for RGB-D cameras.pdf:pdf},
	Isbn = {9781467363587},
	Issn = {21530858},
	Journal = {IEEE International Conference on Intelligent Robots and Systems},
	Pages = {2100--2106},
	Pmid = {6696650},
	Title = {{Dense visual SLAM for RGB-D cameras}},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IROS.2013.6696650}}

@article{Klein2007,
	Abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
	Author = {Klein, Georg and Murray, David},
	Doi = {10.1109/ISMAR.2007.4538852},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Klein, Murray - 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR - 2007 - Parallel tracking and mappi.pdf:pdf},
	Isbn = {9781424417506},
	Issn = {00472778},
	Journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
	Title = {{Parallel tracking and mapping for small AR workspaces}},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISMAR.2007.4538852}}

@article{Koenig2004,
	Abstract = { Simulators have played a critical role in robotics research as tools for quick and efficient testing of new concepts, strategies, and algorithms. To date, most simulators have been restricted to 2D worlds, and few have matured to the point where they are both highly capable and easily adaptable. Gazebo is designed to fill this niche by creating a 3D dynamic multi-robot environment capable of recreating the complex worlds that would be encountered by the next generation of mobile robots. Its open source status, fine grained control, and high fidelity place Gazebo in a unique position to become more than just a stepping stone between the drawing board and real hardware: data visualization, simulation of remote environments, and even reverse engineering of blackbox systems are all possible applications. Gazebo is developed in cooperation with the Player and Stage projects (Gerkey, B. P., et al., July 2003), (Gerkey, B. P., et al., May 2001), (Vaughan, R. T., et al., Oct. 2003), and is available from http://playerstage.sourceforge.net/gazebo/ gazebo.html.},
	Author = {Koenig, N. and Howard, a.},
	Doi = {10.1109/IROS.2004.1389727},
	File = {::},
	Isbn = {0-7803-8463-6},
	Journal = {2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)},
	Pages = {2149--2154},
	Title = {{Design and use paradigms for Gazebo, an open-source multi-robot simulator}},
	Volume = {3},
	Year = {2004},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IROS.2004.1389727}}

@article{Lategahn2011,
	Abstract = {Simultaneous Localization and Mapping (SLAM) and Visual SLAM (V-SLAM) in particular have been an active area of research lately. In V-SLAM the main focus is most often laid on the localization part of the problem allowing for a drift free motion estimate. To this end, a sparse set of landmarks is tracked and their position is estimated. However, this set of landmarks (rendering the map) is often too sparse for tasks in autonomous driving such as navigation, path planning, obstacle avoidance etc. Some methods keep the raw measurements for past robot poses to address the sparsity problem often resulting in a pose only SLAM akin to laser scanner SLAM. For the stereo case, this is however impractical due to the high noise of stereo reconstructed point clouds. In this paper we propose a dense stereo V-SLAM algorithm that estimates a dense 3D map representation which is more accurate than raw stereo measurements. Thereto, we run a sparse V-SLAM system, take the resulting pose estimates to compute a locally dense representation from dense stereo correspondences. This dense representation is expressed in local coordinate systems which are tracked as part of the SLAM estimate. This allows the dense part to be continuously updated. Our system is driven by visual odometry priors to achieve high robustness when tracking landmarks. Moreover, the sparse part of the SLAM system uses recently published sub mapping techniques to achieve constant runtime complexity most of the time. The improved accuracy over raw stereo measurements is shown in a Monte Carlo simulation. Finally, we demonstrate the feasibility of our method by presenting outdoor experiments of a car like robot.},
	Author = {Lategahn, Henning and Geiger, Andreas and Kitt, Bernd},
	Doi = {10.1109/ICRA.2011.5979711},
	File = {::},
	Isbn = {9781612843865},
	Issn = {10504729},
	Journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	Pages = {1732--1737},
	Title = {{Visual SLAM for autonomous ground vehicles}},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICRA.2011.5979711}}

@conference{Lee2012,
	Author = {Lee, JH},
	Booktitle = {Pattern Recognition (ICPR), 2012 21st International Conference of},
	Date-Modified = {2015-12-15 05:47:59 +0000},
	Isbn = {9784990644109},
	Journal = {Pattern Recognition (ICPR), 2012 21st International Conference of},
	Keywords = {2D/3D Object Detection and Recognition,Motion,Scene Understanding,Tracking and Video Analysis},
	Number = {Icpr},
	Pages = {758--762},
	Title = {{Camera calibration from a single image based on coupled line cameras and rectangle constraint}},
	Year = {2012},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/xpls/abs%7B%5C_%7Dall.jsp?arnumber=6460245}}

@inproceedings{Lee2014,
	Abstract = {A new geometric framework, called generalized coupled line camera (GCLC), is proposed to derive an analytic solution to reconstruct an unknown scene quadrilateral and the relevant projective structure from a single or multiple image quadrilaterals. We extend the previous approach developed for rectangle to handle arbitrary scene quadrilaterals. First, we generalize a single line camera by removing the centering constraint that the principal axis should bisect a scene line. Then, we couple a pair of generalized line cameras to model a frustum with a quadrilateral base. Finally, we show that the scene quadrilateral and the center of projection can be analytically reconstructed from a single view when prior knowledge on the quadrilateral is available. A completely unknown quadrilateral can be reconstructed from four views through non-linear optimization. We also describe a improved method to handle an off-centered case by geometrically inferring a centered proxy quadrilateral, which accelerates a reconstruction process without relying on homography. The proposed method is easy to implement since each step is expressed as a simple analytic equation. We present the experimental results on real and synthetic examples.},
	Author = {Lee, Joo-Haeng},
	Booktitle = {2014 22nd International Conference on Pattern Recognition},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Lee - 2014 22nd International Conference on Pattern Recognition - 2014 - New Geometric Interpretation and Analytic Solution for Quadrila.pdf:pdf},
	Issn = {1051-4651},
	Keywords = {Cameras,Couplings,Equations,Image reconstruction,Mathematical model,Periodic structures,Vectors,generalized coupled line camera,geometric interpretation framework,geometry,image reconstruction,nonlinear optimization,optimisation,quadrilateral reconstruction,reconstruction process,scene quadrilateral,simple analytic equation},
	Month = {aug},
	Pages = {4015--4020},
	Publisher = {IEEE},
	Shorttitle = {Pattern Recognition (ICPR), 2014 22nd Internationa},
	Title = {{New Geometric Interpretation and Analytic Solution for Quadrilateral Reconstruction}},
	Url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6977401},
	Year = {2014},
	Bdsk-Url-1 = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6977401}}

@article{Nister2005,
	Abstract = {A system capable of performing robust live ego-motion estimation for perspective cameras is presented. The system is powered by random sample consensus with preemptive scoring of the motion hypotheses. A general statement of the problem of efficient preemptive scoring is given. Then a theoretical investigation of preemptive scoring under a simple inlier-outlier model is performed. A practical preemption scheme is proposed and it is shown that the preemption is powerful enough to enable robust live structure and motion estimation.},
	Author = {Nist{\'{e}}r, David},
	Doi = {10.1007/s00138-005-0006-y},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Nist{\'{e}}r - Machine Vision and Applications - 2005 - Preemptive RANSAC for live structure and motion estimation.pdf:pdf},
	Isbn = {0-7695-1950-4},
	Issn = {09328092},
	Journal = {Machine Vision and Applications},
	Keywords = {3D-Reconstruction,Ego-motion,Real-time,Robust estimation,Structure from motion},
	Number = {5},
	Pages = {321--329},
	Title = {{Preemptive RANSAC for live structure and motion estimation}},
	Volume = {16},
	Year = {2005},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00138-005-0006-y}}

@article{Silveira2008,
	Abstract = {The majority of visual simultaneous localization and mapping (SLAM) approaches consider feature correspondences as an input to the joint process of estimating the camera pose and the scene structure. In this paper, we propose a new approach for simultaneously obtaining the correspondences, the camera pose, the scene structure, and the illumination changes, all directly using image intensities as observations. Exploitation of all possible image information leads to more accurate estimates and avoids the inherent difficulties of reliably associating features. We also show here that, in this case, structural constraints can be enforced within the procedure as well (instead of <i>a</i> <i>posteriori</i>), namely the cheirality, the rigidity, and those related to the lighting variations. We formulate the visual SLAM problem as a nonlinear image alignment task. The proposed parameters to perform this task are optimally computed by an efficient second-order approximation method for fast processing and avoidance of irrelevant minima. Furthermore, a new solution to the visual SLAM initialization problem is described whereby no assumptions are made about either the scene or the camera motion. Experimental results are provided for a variety of scenes, including urban and outdoor ones, under general camera motion and different types of perturbations.},
	Author = {Silveira, G. and Malis, E. and Rives, P.},
	Doi = {10.1109/TRO.2008.2004829},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Silveira, Malis, Rives - IEEE Transactions on Robotics - 2008 - An Efficient Direct Approach to Visual SLAM.pdf:pdf},
	Isbn = {1552-3098},
	Issn = {1552-3098},
	Journal = {IEEE Transactions on Robotics},
	Keywords = {Illumination changes,image registration,structure and motion,vision-based simultaneous localization and mapping},
	Number = {5},
	Pages = {969--979},
	Title = {{An Efficient Direct Approach to Visual SLAM}},
	Volume = {24},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TRO.2008.2004829}}

@article{Strasdat2010,
	Abstract = {While the most accurate solution to off-line structure from motion (SFM) problems is undoubtedly to extract as much correspondence information as possible and perform global optimisation, sequential methods suitable for live video streams must approximate this to fit within fixed computational bounds. Two quite different approaches to real-time SFM - also called monocular SLAM (Simultaneous Localisation and Mapping) - have proven successful, but they sparsify the problem in different ways. Filtering methods marginalise out past poses and summarise the information gained over time with a probability distribution. Keyframe methods retain the optimisation approach of global bundle adjustment, but computationally must select only a small number of past frames to process. In this paper we perform the first rigorous analysis of the relative advantages of filtering and sparse optimisation for sequential monocular SLAM. A series of experiments in simulation as well using a real image SLAM system were performed by means of covariance propagation and Monte Carlo methods, and comparisons made using a combined cost/accuracy measure. With some well-discussed reservations, we conclude that while filtering may have a niche in systems with low processing resources, in most modern applications keyframe optimisation gives the most accuracy per unit of computing time.},
	Author = {Strasdat, Hauke and Montiel, J. M M and Davison, Andrew J.},
	Doi = {10.1109/ROBOT.2010.5509636},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Strasdat, Montiel, Davison - Proceedings - IEEE International Conference on Robotics and Automation - 2010 - Real-time monocular SLAM Wh.pdf:pdf},
	Isbn = {9781424450381},
	Issn = {10504729},
	Journal = {Proceedings - IEEE International Conference on Robotics and Automation},
	Pages = {2657--2664},
	Pmid = {5509636},
	Title = {{Real-time monocular SLAM: Why filter?}},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ROBOT.2010.5509636}}

@article{Tardif2008,
	Abstract = {We present a system for monocular simultaneous localization and mapping (mono-SLAM) relying solely on video input. Our algorithm makes it possible to precisely estimate the camera trajectory without relying on any motion model. The estimation is completely incremental: at a given time frame, only the current location is estimated while the previous camera positions are never modified. In particular, we do not perform any simultaneous iterative optimization of the camera positions and estimated 3D structure (local bundle adjustment). The key aspect of the system is a fast and simple pose estimation algorithm that uses information not only from the estimated 3D map, but also from the epipolar constraint. We show that the latter leads to a much more stable estimation of the camera trajectory than the conventional approach. We perform high precision camera trajectory estimation in urban scenes with a large amount of clutter. Using an omnidirectional camera placed on a vehicle, we cover one of the longest distance ever reported, up to 2.5 kilometers.},
	Author = {Tardif, J.-P. and Pavlidis, Y. and Daniilidis, K.},
	Doi = {10.1109/IROS.2008.4651205},
	File = {:Users/jaemin/Documents/Mendeley Desktop/Tardif, Pavlidis, Daniilidis - 2008 IEEERSJ International Conference on Intelligent Robots and Systems - 2008 - Monocular visual odometr.pdf:pdf},
	Isbn = {978-1-4244-2057-5},
	Journal = {2008 IEEE/RSJ International Conference on Intelligent Robots and Systems},
	Keywords = {Computer Vision,Mapping,SLAM},
	Pages = {22--26},
	Title = {{Monocular visual odometry in urban environments using an omnidirectional camera}},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IROS.2008.4651205}}
